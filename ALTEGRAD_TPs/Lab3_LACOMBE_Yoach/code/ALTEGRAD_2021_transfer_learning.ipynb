{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_2021_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "# Transfer learning for NLP\n",
        "## ALTEGRAD - Lab session 3\n",
        "#### Moussa Kamal Eddine, Hadi Abdine (Dascim LIX)\n",
        "##### 23 November 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        '''\n",
        "        ntokens: the size of vocabulary\n",
        "        nhid: the hidden dimension of the model.\n",
        "        We assume that embedding_dim = nhid\n",
        "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        nhead: the number of heads in the multiheadattention models\n",
        "        dropout: the dropout value\n",
        "         '''\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.encoder = nn.Embedding(ntokens, nhid) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid, dropout = dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        encoder_layers = nn.TransformerEncoderLayer(nhid, nhead, nhid) #fill me we assume nhid = d_model = dim_feedforward\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers) #fill me\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)#fill me\n",
        "        return output\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid, nclasses):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "    \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers, dropout)\n",
        "        self.classifier = ClassificationHead(nhid, nclasses)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # base model\n",
        "        x = self.base(src, src_mask)\n",
        "        # classifier model\n",
        "        output = self.classifier(x)#fill me\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, nhid)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEYHJx2JW6l"
      },
      "source": [
        "Let's verify if our model works, by applying one inference step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhb2gkUhJMR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a93ee0a-3b41-4fdf-ce52-b1b32d2e4237"
      },
      "source": [
        "ntokens = 100 # the size of vocabulary\n",
        "nhid = 200  # hidden dimension\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
        "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
        "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
        "out = model.forward(dummy_input, src_mask)\n",
        "\n",
        "print(out.shape) # is it the right shape?"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qjd26ghWuff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34f094c-b651-46ed-94ae-7f85d71d6838"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!head -5 dict.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:30:14--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577587 (564K) [text/plain]\n",
            "Saving to: ‘dict.txt.11’\n",
            "\n",
            "dict.txt.11         100%[===================>] 564.05K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-11-25 16:30:14 (13.0 MB/s) - ‘dict.txt.11’ saved [577587/577587]\n",
            "\n",
            "▁d 1\n",
            "es 1\n",
            "▁l 1\n",
            "en 1\n",
            "on 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFdH_-JeFbGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49885041-1252-4cce-bed4-81c7920543bb"
      },
      "source": [
        "path_vocab = \"dict.txt\"\n",
        "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
        "i = 0\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] =  4 + i #fill me\n",
        "        i+=1\n",
        "\n",
        "ind2token = {v: k for k, v in token2ind.items()} #fill me\n",
        "\n",
        "print(ind2token[1111])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁trop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents,\n",
        "        path_labels=None,\n",
        "        token2ind={},\n",
        "        max_len=512,\n",
        "        task=\"language_modeling\",\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1]\n",
        "        def inter(x):\n",
        "          if x in self.token2ind:\n",
        "            return self.token2ind[x]\n",
        "          else:\n",
        "            return 3\n",
        "        source_sequence = [0] + [inter(s) for s in sequence]#fill me (constract the input sequence using token2ind, sequence and special tokens)\n",
        "        \n",
        "        if self.task == \"language_modeling\":\n",
        "            target = source_sequence[1:]\n",
        "            target.append(self.token2ind[\"<eos>\"])\n",
        "        elif self.task == \"classification\":\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            \"source_sequence\": torch.tensor(source_sequence),\n",
        "            \"target\": torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def MyCollator(batch):\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    path_documents,\n",
        "    path_labels=None,\n",
        "    token2ind={},\n",
        "    max_len=512,\n",
        "    batch_size=32,\n",
        "    task=\"language_modeling\",\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=512,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=MyCollator,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "source": [
        "import gc\n",
        "\n",
        "def train(\n",
        "    path_data_train,\n",
        "    path_labels_train=None,\n",
        "    path_data_valid=None,\n",
        "    save_interval=-1,\n",
        "    log_interval=5,\n",
        "    task=\"language_modeling\",\n",
        "    batch_size=32,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    \n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): #step 1\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask) #step 2\n",
        "        if task == 'classification':\n",
        "            #last vector only\n",
        "            output = output[-1,:,:] #fill me \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target =  data[1]#fill me\n",
        "        target = target.to(device)\n",
        "        loss =  criterion(output, target)#fill me, Cross entropy check next cells\n",
        "        #fill me step 3\n",
        "\n",
        "        \n",
        "        #fill me step 4\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        total_loss += loss.item() \n",
        "\n",
        "        #gc.collect()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
        "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
        "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
        "                )\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    return losses"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgf6BDB9jUr6"
      },
      "source": [
        "ntokens = len(token2ind)#fill me # the size of vocabulary\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-OLy4KIkDwf"
      },
      "source": [
        "# optimization paramerters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwh3n9xZQy4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45acf8e-39b2-4de9-ec73-a23c3c997b32"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "path_data_train = \"pretraining_subset.txt\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:30:15--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10146460 (9.7M) [text/plain]\n",
            "Saving to: ‘pretraining_subset.txt.15’\n",
            "\n",
            "pretraining_subset. 100%[===================>]   9.68M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-11-25 16:30:16 (92.9 MB/s) - ‘pretraining_subset.txt.15’ saved [10146460/10146460]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m11g4ScjZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69251b87-7740-4681-a411-a09831899ba2"
      },
      "source": [
        "#pretraining on a tiny subset\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        path_data_train,\n",
        "        save_interval=-1,\n",
        "        task='language_modeling', # fill me\n",
        "        batch_size=16,\n",
        "        log_interval=log_interval,\n",
        "    )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 3125 steps | loss 7.35894 | ppl 1570.171\n",
            "| epoch   1 |  1000/ 3125 steps | loss 6.54525 | ppl  695.930\n",
            "| epoch   1 |  1500/ 3125 steps | loss 6.26372 | ppl  525.170\n",
            "| epoch   1 |  2000/ 3125 steps | loss 6.09785 | ppl  444.900\n",
            "| epoch   1 |  2500/ 3125 steps | loss 5.97205 | ppl  392.308\n",
            "| epoch   1 |  3000/ 3125 steps | loss 5.86498 | ppl  352.476\n",
            "| epoch   2 |   500/ 3125 steps | loss 5.58203 | ppl  265.610\n",
            "| epoch   2 |  1000/ 3125 steps | loss 5.52349 | ppl  250.508\n",
            "| epoch   2 |  1500/ 3125 steps | loss 5.51330 | ppl  247.968\n",
            "| epoch   2 |  2000/ 3125 steps | loss 5.45860 | ppl  234.769\n",
            "| epoch   2 |  2500/ 3125 steps | loss 5.42819 | ppl  227.736\n",
            "| epoch   2 |  3000/ 3125 steps | loss 5.40669 | ppl  222.893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BcBC6FSkMH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c053b7-cd7d-4409-d877-eefb552e2e30"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device) \n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
        "#load state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict']) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:41:52--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88093955 (84M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_model_4layers.pt.1’\n",
            "\n",
            "pretrained_model_4l 100%[===================>]  84.01M   144MB/s    in 0.6s    \n",
            "\n",
            "2021-11-25 16:41:54 (144 MB/s) - ‘pretrained_model_4layers.pt.1’ saved [88093955/88093955]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBRRVsWqlIoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66e1596-3bed-45dc-c30d-6d68892bf4cd"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "--2021-11-25 16:41:58--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115362 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘sentencepiece.french.model.1’\n",
            "\n",
            "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-11-25 16:41:58 (19.8 MB/s) - ‘sentencepiece.french.model.1’ saved [1115362/1115362]\n",
            "\n",
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "source": [
        "def infer_next_token(sent):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    \n",
        "    next_token_ind = nn.functional.softmax(out[-1,:,:].squeeze(), dim = 0)\n",
        "    next_token_ind =  torch.argmax(next_token_ind).item()#fill me\n",
        "    return next_token_ind, out\n",
        "    \n",
        "def infer_next_tokens(sent, max_len=50):\n",
        "    # to be implemented\n",
        "    for i in range(max_len):\n",
        "      next_token_ind, out = infer_next_token(sent)\n",
        "      sent = sent + ' ' + ind2token[next_token_ind]\n",
        "      if next_token_ind == token2ind['<eos>']:\n",
        "        return sent"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f83Nn5nSly4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fa8657c-2d55-4a3e-b782-94d4d9e75123"
      },
      "source": [
        "sent = \"Bonjour les\"\n",
        "infer_next_tokens(sent)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bonjour les ▁gens ▁qui ▁ont ▁été ▁très ▁accueillants ▁et ▁sympathiques . <eos>'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K1BZsblmEmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5415cdf-6198-4fef-b904-94d2768d4898"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "\n",
        "path_data_train = \"train.review.spm\"\n",
        "path_labels_train = \"train.label\"\n",
        "\n",
        "path_data_valid = \"test.review.spm\"\n",
        "path_labels_valid = \"test.label\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:41:58--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1495960 (1.4M) [text/plain]\n",
            "Saving to: ‘train.review.spm.1’\n",
            "\n",
            "train.review.spm.1  100%[===================>]   1.43M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-11-25 16:41:59 (26.4 MB/s) - ‘train.review.spm.1’ saved [1495960/1495960]\n",
            "\n",
            "--2021-11-25 16:41:59--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [text/plain]\n",
            "Saving to: ‘train.label.1’\n",
            "\n",
            "train.label.1       100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-25 16:41:59 (45.1 MB/s) - ‘train.label.1’ saved [3200/3200]\n",
            "\n",
            "--2021-11-25 16:41:59--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864544 (1.8M) [text/plain]\n",
            "Saving to: ‘test.review.spm.1’\n",
            "\n",
            "test.review.spm.1   100%[===================>]   1.78M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-11-25 16:41:59 (31.5 MB/s) - ‘test.review.spm.1’ saved [1864544/1864544]\n",
            "\n",
            "--2021-11-25 16:41:59--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4000 (3.9K) [text/plain]\n",
            "Saving to: ‘test.label.1’\n",
            "\n",
            "test.label.1        100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-25 16:42:00 (41.6 MB/s) - ‘test.label.1’ saved [4000/4000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader):\n",
        "    #to be implemented\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    for idx, data in enumerate(data_loader):\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask)\n",
        "        \n",
        "        output = output[-1,:,:] \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "\n",
        "        output = nn.functional.softmax(output,dim = 1)\n",
        "        output = torch.argmax(output, 1)\n",
        "        target =  data[1].to(device)\n",
        "        target = target\n",
        "\n",
        "        train_acc = torch.sum(output == target)\n",
        "\n",
        "        accuracy += train_acc.item()\n",
        "    return accuracy/len(data_loader)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xclMCpnVpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e39c63-e412-4d43-a97f-574cf1110e67"
      },
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            path_data_train,\n",
        "            path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task='classification',\n",
        "            batch_size=8,\n",
        "            log_interval=50,\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            get_loader(\n",
        "                path_data_valid,\n",
        "                path_labels_valid,\n",
        "                token2ind=token2ind,\n",
        "                batch_size=20,\n",
        "                task='classification',\n",
        "            )\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.74856 | ppl    2.114\n",
            "| epoch   1 |   100/  200 steps | loss 0.72820 | ppl    2.071\n",
            "| epoch   1 |   150/  200 steps | loss 0.71647 | ppl    2.047\n",
            "| epoch   2 |    50/  200 steps | loss 0.69051 | ppl    1.995\n",
            "| epoch   2 |   100/  200 steps | loss 0.59589 | ppl    1.815\n",
            "| epoch   2 |   150/  200 steps | loss 0.65972 | ppl    1.934\n",
            "| epoch   3 |    50/  200 steps | loss 0.36302 | ppl    1.438\n",
            "| epoch   3 |   100/  200 steps | loss 0.36607 | ppl    1.442\n",
            "| epoch   3 |   150/  200 steps | loss 0.33866 | ppl    1.403\n",
            "| epoch   4 |    50/  200 steps | loss 0.12445 | ppl    1.133\n",
            "| epoch   4 |   100/  200 steps | loss 0.17396 | ppl    1.190\n",
            "| epoch   4 |   150/  200 steps | loss 0.15130 | ppl    1.163\n",
            "| epoch   5 |    50/  200 steps | loss 0.03864 | ppl    1.039\n",
            "| epoch   5 |   100/  200 steps | loss 0.01160 | ppl    1.012\n",
            "| epoch   5 |   150/  200 steps | loss 0.05370 | ppl    1.055\n",
            "| epoch   6 |    50/  200 steps | loss 0.00036 | ppl    1.000\n",
            "| epoch   6 |   100/  200 steps | loss 0.00542 | ppl    1.005\n",
            "| epoch   6 |   150/  200 steps | loss 0.00104 | ppl    1.001\n",
            "| epoch   7 |    50/  200 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   7 |   100/  200 steps | loss 0.01122 | ppl    1.011\n",
            "| epoch   7 |   150/  200 steps | loss 0.00012 | ppl    1.000\n",
            "| epoch   8 |    50/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |   150/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   150/  200 steps | loss 0.00020 | ppl    1.000\n",
            "| epoch  10 |    50/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00310 | ppl    1.003\n",
            "| epoch  11 |    50/  200 steps | loss 0.00264 | ppl    1.003\n",
            "| epoch  11 |   100/  200 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |    50/  200 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.00051 | ppl    1.001\n",
            "| epoch  13 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/  200 steps | loss 0.82734 | ppl    2.287\n",
            "| epoch   1 |   100/  200 steps | loss 0.72199 | ppl    2.059\n",
            "| epoch   1 |   150/  200 steps | loss 0.68015 | ppl    1.974\n",
            "| epoch   2 |    50/  200 steps | loss 0.50496 | ppl    1.657\n",
            "| epoch   2 |   100/  200 steps | loss 0.52208 | ppl    1.686\n",
            "| epoch   2 |   150/  200 steps | loss 0.45083 | ppl    1.570\n",
            "| epoch   3 |    50/  200 steps | loss 0.38868 | ppl    1.475\n",
            "| epoch   3 |   100/  200 steps | loss 0.42043 | ppl    1.523\n",
            "| epoch   3 |   150/  200 steps | loss 0.44753 | ppl    1.564\n",
            "| epoch   4 |    50/  200 steps | loss 0.31267 | ppl    1.367\n",
            "| epoch   4 |   100/  200 steps | loss 0.26870 | ppl    1.308\n",
            "| epoch   4 |   150/  200 steps | loss 0.27731 | ppl    1.320\n",
            "| epoch   5 |    50/  200 steps | loss 0.22992 | ppl    1.258\n",
            "| epoch   5 |   100/  200 steps | loss 0.22497 | ppl    1.252\n",
            "| epoch   5 |   150/  200 steps | loss 0.28000 | ppl    1.323\n",
            "| epoch   6 |    50/  200 steps | loss 0.18801 | ppl    1.207\n",
            "| epoch   6 |   100/  200 steps | loss 0.11260 | ppl    1.119\n",
            "| epoch   6 |   150/  200 steps | loss 0.27645 | ppl    1.318\n",
            "| epoch   7 |    50/  200 steps | loss 0.13716 | ppl    1.147\n",
            "| epoch   7 |   100/  200 steps | loss 0.03592 | ppl    1.037\n",
            "| epoch   7 |   150/  200 steps | loss 0.06606 | ppl    1.068\n",
            "| epoch   8 |    50/  200 steps | loss 0.02107 | ppl    1.021\n",
            "| epoch   8 |   100/  200 steps | loss 0.05834 | ppl    1.060\n",
            "| epoch   8 |   150/  200 steps | loss 0.07859 | ppl    1.082\n",
            "| epoch   9 |    50/  200 steps | loss 0.01390 | ppl    1.014\n",
            "| epoch   9 |   100/  200 steps | loss 0.04296 | ppl    1.044\n",
            "| epoch   9 |   150/  200 steps | loss 0.01859 | ppl    1.019\n",
            "| epoch  10 |    50/  200 steps | loss 0.00141 | ppl    1.001\n",
            "| epoch  10 |   100/  200 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00011 | ppl    1.000\n",
            "| epoch  11 |    50/  200 steps | loss 0.00730 | ppl    1.007\n",
            "| epoch  11 |   100/  200 steps | loss 0.00023 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00111 | ppl    1.001\n",
            "| epoch  12 |    50/  200 steps | loss 0.00020 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  13 |    50/  200 steps | loss 0.00023 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.01398 | ppl    1.014\n",
            "| epoch  13 |   150/  200 steps | loss 0.01197 | ppl    1.012\n",
            "| epoch  14 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00332 | ppl    1.003\n",
            "| epoch  15 |    50/  200 steps | loss 0.00486 | ppl    1.005\n",
            "| epoch  15 |   100/  200 steps | loss 0.02595 | ppl    1.026\n",
            "| epoch  15 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCpBIdTHojm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cc40f9b0-2713-4303-851d-6bae187aad47"
      },
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(from_scratch_valid_acc, label = 'From Scratch')\n",
        "\n",
        "plt.plot(pretrained_valid_acc, label = 'From pretrained model')\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9bOeqpj0yLB",
        "outputId": "aa6c137e-01af-4434-8545-bc89ff28c10e"
      },
      "source": [
        "sum_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"nb trainable parameters\", sum_param)\n",
        "\n",
        "model\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (base): TransformerModel(\n",
              "    (encoder): Embedding(50001, 200)\n",
              "    (pos_encoder): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (transformer_encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): ClassificationHead(\n",
              "    (decoder): Linear(in_features=200, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-keo3xdDAZdn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}